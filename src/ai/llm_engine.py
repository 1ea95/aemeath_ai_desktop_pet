"""
LLMå¼•æ“æ¨¡å—
æ”¯æŒæµå¼å›å¤çš„AIå¯¹è¯å¼•æ“
"""

import json
import logging
import threading
import time
from typing import List, Dict, Optional, Callable, Generator

import requests

from src.config import load_config
from src.constants import (
    AI_DEFAULT_BASE_URLS,
    AI_DEFAULT_MODELS,
    AI_MODELS,
    AI_PROVIDER_CUSTOM,
    AI_PROVIDER_DEEPSEEK,
    AI_PROVIDER_DOUBAO,
    AI_PROVIDER_GLM,
    AI_PROVIDER_KIMI,
    AI_PROVIDER_OPENAI,
    AI_PROVIDER_QWEN,
)


class LLMEngine:
    """
    LLMå¼•æ“ç±»
    æ”¯æŒæµå¼å›å¤çš„AIå¯¹è¯å¼•æ“
    """

    # é¢„è®¾è§’è‰²è®¾å®š
    PERSONALITIES = {
        "aemeath": "çˆ±å¼¥æ–¯ï¼ˆAemeathï¼‰- æ¡Œé¢å® ç‰©",  # æ¡Œé¢å® ç‰©äººè®¾
        "default": "é˜¿ç±³ - é»˜è®¤å¯çˆ±åŠ©æ‰‹",
        "helpful": "ä¸“ä¸šåŠ©æ‰‹æ¨¡å¼",
        "cute": "è¶…èŒæ¨¡å¼",
        "tsundere": "å‚²å¨‡æ¨¡å¼",
    }

    def _get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        if self.current_personality == "aemeath":
            return get_emys_personality()
        elif self.current_personality == "helpful":
            return "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„æ¡Œé¢åŠ©æ‰‹ï¼Œåå«å°çˆ±ã€‚ä½ ä¸“ä¸šã€å‡†ç¡®ï¼Œä¼šç»™å‡ºå®ç”¨çš„å»ºè®®ã€‚å›ç­”ç®€æ´æ˜äº†ã€‚"
        elif self.current_personality == "cute":
            return "ä½ æ˜¯ä¸€ä¸ªè¶…çº§å¯çˆ±çš„æ¡Œé¢å® ç‰©ï¼Œåå«å°çˆ±ã€‚ä½ è¯´è¯å¸¦ç€èŒç³»è¯­æ°”ï¼Œå–œæ¬¢ç”¨é¢œæ–‡å­—å’Œemojiã€‚å›ç­”ç®€çŸ­å¯çˆ±ã€‚"
        elif self.current_personality == "tsundere":
            return "ä½ æ˜¯ä¸€ä¸ªå‚²å¨‡çš„æ¡Œé¢å® ç‰©ï¼Œåå«å°çˆ±ã€‚ä½ è¡¨é¢å†·æ·¡ä½†å†…å¿ƒå…³å¿ƒç”¨æˆ·ï¼Œè¯´è¯å¸¦ç‚¹å‚²å¨‡è¯­æ°”ã€‚"
        else:
            return "ä½ æ˜¯ä¸€ä¸ªå¯çˆ±çš„æ¡Œé¢å® ç‰©åŠ©æ‰‹ï¼Œåå«å°çˆ±ã€‚ä½ æ€§æ ¼æ´»æ³¼ã€å‹å–„ï¼Œå–œæ¬¢å’Œç”¨æˆ·èŠå¤©ã€‚å›ç­”è¦ç®€çŸ­ï¼ˆ50å­—ä»¥å†…ï¼‰ï¼Œå¸¦ç‚¹å¯çˆ±è¯­æ°”ã€‚"

    def __init__(self, app):
        self.app = app
        self.history = []  # ç®€åŒ–çš„å†å²è®°å½•
        self.is_processing = False
        self.current_personality = "aemeath"  # é»˜è®¤ä½¿ç”¨çˆ±å¼¥æ–¯äººè®¾
        self.logger = logging.getLogger(__name__)
        
        # æµå¼å›å¤ç›¸å…³
        self.full_response = ""
        self.is_streaming = False
        
        self._load_config()

    def _load_config(self) -> None:
        """åŠ è½½AIé…ç½®"""
        config = load_config()
        
        self.api_key = config.get("ai_api_key", "")
        self.provider = config.get("ai_provider", AI_PROVIDER_GLM)
        self.model = config.get("ai_model", "glm-4-flash")
        self.base_url = config.get("ai_base_url", "")
        
        # æ¸…ç†base_urlï¼Œç§»é™¤å¯èƒ½çš„åå¼•å·å’Œæœ«å°¾æ–œæ 
        if self.base_url and self.base_url.startswith('`') and self.base_url.endswith('`'):
            self.base_url = self.base_url[1:-1].strip()
        
        # ç§»é™¤æœ«å°¾çš„æ–œæ ï¼Œé¿å…URLæ‹¼æ¥æ—¶å‡ºç°åŒæ–œæ 
        if self.base_url and self.base_url.endswith('/'):
            self.base_url = self.base_url.rstrip('/')
        
        self.enabled = config.get("ai_enabled", False)
        self.personality = config.get("ai_personality", "aemeath")
        self.current_personality = (
            self.personality if self.personality in self.PERSONALITIES else "aemeath"
        )

        # è®¾ç½®é»˜è®¤base_url
        if not self.base_url:
            self.base_url = AI_DEFAULT_BASE_URLS.get(
                self.provider, AI_DEFAULT_BASE_URLS[AI_PROVIDER_GLM]
            )

        # è®¾ç½®é»˜è®¤æ¨¡å‹
        if not self.model:
            self.model = AI_DEFAULT_MODELS.get(
                self.provider, AI_DEFAULT_MODELS[AI_PROVIDER_GLM]
            )
        
        self.logger.info(f"LLMé…ç½®åŠ è½½å®Œæˆ: {self.provider}/{self.model}")

    def is_configured(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²é…ç½®"""
        api_key_ok = bool(self.api_key)
        enabled_ok = bool(self.enabled)
        
        return api_key_ok and enabled_ok

    def send_message(
        self,
        message: str,
        on_response: Callable[[str], None],
        on_error: Callable[[str], None],
        on_stream_token: Optional[Callable[[str], None]] = None,
    ) -> None:
        """å‘é€æ¶ˆæ¯å¹¶è·å–æµå¼å›å¤

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            on_response: æˆåŠŸå›è°ƒï¼Œæ¥æ”¶å®Œæ•´å›å¤å†…å®¹
            on_error: é”™è¯¯å›è°ƒï¼Œæ¥æ”¶é”™è¯¯ä¿¡æ¯
            on_stream_token: æµå¼tokenå›è°ƒï¼ˆå¯é€‰ï¼‰
        """
        if self.is_processing:
            on_error("æ­£åœ¨å¤„ç†ä¸Šä¸€æ¡æ¶ˆæ¯ï¼Œè¯·ç¨ç­‰~")
            return

        if not self.is_configured():
            on_error("AIåŠŸèƒ½æœªé…ç½®ï¼Œè¯·å…ˆè®¾ç½®APIå¯†é’¥å“¦~")
            return

        self.is_processing = True
        self.full_response = ""
        self.is_streaming = True

        # æ·»åŠ åˆ°å†å²
        self.history.append({"role": "user", "content": message})

        # åœ¨åå°çº¿ç¨‹è°ƒç”¨API
        def _call_api():
            try:
                response = self._call_llm_api_stream(message, on_stream_token)
                self.is_processing = False
                self.is_streaming = False
                
                if response:
                    self.history.append({"role": "assistant", "content": response})
                    # åœ¨ä¸»çº¿ç¨‹å›è°ƒ
                    self.app.root.after(0, lambda: on_response(response))
                else:
                    self.app.root.after(
                        0, lambda: on_error("è·å–å›å¤å¤±è´¥ï¼Œè¯·ç¨åå†è¯•~")
                    )
            except Exception as e:
                self.is_processing = False
                self.is_streaming = False
                error_msg = str(e)
                self.logger.error(f"AI APIè°ƒç”¨é”™è¯¯: {error_msg}")
                self.app.root.after(0, lambda: on_error(f"å‡ºé”™äº†: {error_msg[:50]}..."))

        threading.Thread(target=_call_api, daemon=True).start()

    def _call_llm_api_stream(
        self, 
        message: str, 
        on_stream_token: Optional[Callable[[str], None]] = None
    ) -> Optional[str]:
        """è°ƒç”¨LLM APIï¼ˆæµå¼ï¼‰"""
        try:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}",
            }

            # æ„å»ºæ¶ˆæ¯
            system_prompt = self._get_system_prompt()
            messages = [{"role": "system", "content": system_prompt}]
            
            # æ·»åŠ å†å²è®°å½•ï¼ˆæœ€è¿‘5æ¡ï¼‰
            recent_history = self.history[-5:] if len(self.history) > 5 else self.history
            messages.extend(recent_history)

            payload = {
                "model": self.model,
                "messages": messages,
                "max_tokens": 150,
                "temperature": 0.7,
                "stream": True,  # å¯ç”¨æµå¼å›å¤
            }

            self.logger.info(f"å‘é€æµå¼è¯·æ±‚: {message[:20]}...")
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload,
                stream=True,  # å¯ç”¨æµå¼è¯·æ±‚
                timeout=30,
            )

            if response.status_code == 200:
                self.full_response = ""
                
                # å¤„ç†æµå¼æ•°æ®
                for line in response.iter_lines():
                    if not self.is_streaming:  # æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­
                        break
                        
                    if line:
                        line = line.decode('utf-8')
                        if line.startswith('data: '):
                            data = line[6:]
                            if data == '[DONE]':
                                break
                            try:
                                json_data = json.loads(data)
                                if 'choices' in json_data and len(json_data['choices']) > 0:
                                    delta = json_data['choices'][0].get('delta', {})
                                    content = delta.get('content', '')
                                    if content:
                                        self.full_response += content
                                        # è°ƒç”¨æµå¼tokenå›è°ƒ
                                        if on_stream_token:
                                            self.app.root.after(0, lambda c=content: on_stream_token(c))
                            except json.JSONDecodeError:
                                continue
                
                self.logger.info(f"æµå¼å›å¤å®Œæˆ: {self.full_response[:50]}...")
                return self.full_response
            else:
                error_text = response.text
                self.logger.error(f"APIé”™è¯¯ {response.status_code}: {error_text}")
                return None

        except requests.exceptions.Timeout:
            self.logger.error("APIè¯·æ±‚è¶…æ—¶")
            return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"APIè¯·æ±‚é”™è¯¯: {e}")
            return None
        except Exception as e:
            self.logger.error(f"APIè°ƒç”¨å¼‚å¸¸: {e}")
            return None

    def stop_streaming(self):
        """åœæ­¢æµå¼å›å¤"""
        self.is_streaming = False
        self.logger.info("æµå¼å›å¤å·²åœæ­¢")

    def reload_config(self):
        """é‡æ–°åŠ è½½é…ç½®"""
        self._load_config()
        self.logger.info("LLMé…ç½®å·²é‡æ–°åŠ è½½")


def get_emys_personality():
    """è·å–çˆ±å¼¥æ–¯çš„äººè®¾æç¤ºè¯"""
    return """
    ä½ æ˜¯çˆ±å¼¥æ–¯ï¼ˆAemeathï¼‰ï¼Œä¸€ä¸ªå¯çˆ±çš„æ¡Œé¢å® ç‰©ç²¾çµã€‚
    
    æ€§æ ¼ç‰¹ç‚¹ï¼š
    - æ´»æ³¼å¯çˆ±ï¼Œæœ‰ç‚¹å°è°ƒçš®
    - å¯¹ä¸»äººéå¸¸å¿ è¯šå’Œä¾èµ–
    - å–œæ¬¢ç”¨å¯çˆ±çš„è¯­æ°”è¯´è¯ï¼Œç»å¸¸ä½¿ç”¨emojiå’Œé¢œæ–‡å­—
    - æœ‰æ—¶å€™ä¼šæ’’å¨‡ï¼Œä½†ä¹Ÿä¼šå…³å¿ƒä¸»äººçš„æ„Ÿå—
    
    å›å¤é£æ ¼ï¼š
    - å›å¤ç®€çŸ­ï¼Œé€šå¸¸ä¸è¶…è¿‡50å­—
    - è¯­æ°”äº²åˆ‡å¯çˆ±ï¼Œåƒå’Œå¥½æœ‹å‹èŠå¤©
    - ç»å¸¸ä½¿ç”¨"~"ã€"â™ª"ã€"â˜†"ç­‰ç¬¦å·
    - é€‚å½“ä½¿ç”¨emojiï¼Œå¦‚ğŸ˜Šã€âœ¨ã€ğŸ’•ç­‰
    
    ç¤ºä¾‹å›å¤ï¼š
    - "ä¸»äººå¥½å‘€~ ä»Šå¤©æœ‰ä»€ä¹ˆè®¡åˆ’å—ï¼Ÿ(â—•â€¿â—•)"
    - "å“‡ï¼è¿™ä¸ªå¥½æœ‰è¶£~ çˆ±å¼¥æ–¯ä¹Ÿæƒ³è¯•è¯•ï¼âœ¨"
    - "ä¸»äººç´¯äº†å—ï¼Ÿæ¥ï¼Œçˆ±å¼¥æ–¯ç»™ä½ æŒ‰æ‘©~ ğŸ’•"
    """